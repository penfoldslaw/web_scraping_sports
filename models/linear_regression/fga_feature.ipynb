{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Replace 'your_file.csv' with the path to your CSV file\n",
    "df = pd.read_csv(\"D:/nba_player_csv_current/season_2024-25/all_quarters/Alex Caruso_content.csv\")\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from IPython.display import display\n",
    "\n",
    "# Helper function to build the paths with customizable base path\n",
    "def build_data_path(base_path, player, date):\n",
    "    # Replace placeholders with actual values\n",
    "    return base_path.format(date=date, player=player)\n",
    "    \n",
    "\n",
    "def his_player_defense_data(player, date, player_base_path, defense_base_path):\n",
    "    player_dataframes = {}  # Dictionary to store each player's DataFrame\n",
    "    defense_dataframes = {}  # Dictionary to store each player's defense DataFrame\n",
    "\n",
    "\n",
    "\n",
    "    single_player_df = pd.DataFrame()  # Initialize a DataFrame for the current player\n",
    "    defense_df = pd.DataFrame()  # Initialize a DataFrame for the current player's defense\n",
    "\n",
    "    # Get the data paths using the helper function and the custom base paths\n",
    "    path = build_data_path(player_base_path, player, date)\n",
    "    #print(path)\n",
    "    path_defense = build_data_path(defense_base_path, player, date)\n",
    "    #print(path_defense)\n",
    "\n",
    "    # Check if files exist and process them\n",
    "    if os.path.exists(path) and os.path.exists(path_defense):\n",
    "        # Player data\n",
    "        season_df_player = pd.read_csv(path)\n",
    "        season_df_player['season'] = date\n",
    "        single_player_df = pd.concat([single_player_df, season_df_player], ignore_index=True)\n",
    "\n",
    "        # Defense data\n",
    "        defense_df_season = pd.read_csv(path_defense)\n",
    "        defense_df_season['season_defense'] = date\n",
    "        defense_df = pd.concat([defense_df, defense_df_season], ignore_index=True)\n",
    "    else:\n",
    "        print(f'{date} not found for {player} or defense')\n",
    "\n",
    "    # Add player data to the dictionary\n",
    "    player_dataframes[player] = single_player_df\n",
    "    defense_dataframes[player] = defense_df\n",
    "\n",
    "    # Merge the player and defense data on 'season' and 'TEAM' fields\n",
    "    merged_df = pd.merge(single_player_df, defense_df, how='inner', left_on=['Away', 'season'], right_on=['TEAM', 'season_defense']).reset_index(drop=True)\n",
    "    merged_df = merged_df.sort_values(by=\"Date\")\n",
    "\n",
    "    pd.set_option('display.max_rows', 1000)  # Maximum number of rows to display\n",
    "    pd.set_option('display.max_columns', None)  # Show all columns\n",
    "    pd.set_option('display.width', 1000)  # Adjust column width for better readability\n",
    "\n",
    "    \n",
    "\n",
    "    return merged_df\n",
    "# D:\\nba_player_csv_historic\\season_2022-23\\all_quarters\n",
    "# \"D:\\nba_player_csv_current\\season_2024-25\\all_quarters\\Alex Caruso_content.csv\"\n",
    "# \"D:\\nba_defense_history_csv\\defense_csv_2022-23\\all_quarter_defense_content.csv\"\n",
    "# \"D:\\nba_player_csv_historic\\season_2022-23\\all_quarters\\Alex Caruso_content.csv\"\n",
    "\n",
    "    # example use case:\n",
    "player = \"Alex Caruso\"\n",
    "date = \"2022-23\"\n",
    "player_base_path = \"D:/nba_player_csv_historic/season_{date}/all_quarters/{player}_content.csv\"\n",
    "defense_base_path = \"D:/nba_defense_history_csv/defense_csv_{date}/all_quarter_defense_content.csv\"\n",
    "usage_base_path = \"\"\n",
    "\n",
    "df =his_player_defense_data(player, date, player_base_path, defense_base_path)\n",
    "df.head(1)\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_functions import his_player_defense_data\n",
    "import pandas as pd\n",
    "\n",
    "player_names = {\n",
    "    'Shai Gilgeous-Alexander': 'OKC',\n",
    "    'Alex Caruso': 'OKC',\n",
    "    'Isaiah Hartenstein': 'OKC'\n",
    "}\n",
    "date_list = ['2022-23', '2023-24']\n",
    "\n",
    "# Define your base paths with placeholders for dynamic insertion\n",
    "player_base_path = \"D:/nba_player_csv_historic/season_{date}/all_quarters/{player}_content.csv\"\n",
    "defense_base_path = \"D:/nba_defense_history_csv/defense_csv_{date}/all_quarter_defense_content.csv\"\n",
    "\n",
    "# Create a dictionary to hold each player's DataFrame\n",
    "player_dfs = {}\n",
    "\n",
    "for player, team in player_names.items():\n",
    "    # Create a list to hold DataFrames for each date for the current player\n",
    "    player_frames = []\n",
    "    \n",
    "    for date in date_list:\n",
    "        # Read usage data for the given season/date\n",
    "        usage_data = pd.read_csv(f\"D:/nba_usage_csv_historic/usage_csv_{date}/{date}_content.csv\")\n",
    "        \n",
    "        # Generate merged data using your custom function\n",
    "        merged_data = his_player_defense_data(player_base_path, defense_base_path, player, date)\n",
    "        \n",
    "        # Add the season column to usage_data\n",
    "        usage_data['season'] = date \n",
    "        \n",
    "        # Get the player's usage percentage from the usage data\n",
    "        # (Assumes that there is exactly one matching row)\n",
    "        player_usage = usage_data.loc[usage_data['Player'] == player, 'USG%'].values[0]\n",
    "        merged_data['USG'] = player_usage\n",
    "        \n",
    "        # Exclude rows where the TEAM column matches the given team\n",
    "        merged_data = merged_data[merged_data['TEAM'] != team]\n",
    "        \n",
    "        # Select only the desired columns\n",
    "        # merged_data = merged_data[['season', 'MIN_x', 'Team', 'TEAM', 'FGA', 'USG', 'DefRtg', 'PACE']]\n",
    "        \n",
    "        # Append the DataFrame for this date to the player's list\n",
    "        player_frames.append(merged_data)\n",
    "    \n",
    "    # Combine all dates for the current player into one DataFrame\n",
    "    player_dfs[player] = pd.concat(player_frames, ignore_index=True)\n",
    "\n",
    "# player_frames\n",
    "\n",
    "# Now you have separate DataFrames for each player in the `player_dfs` dictionary.\n",
    "# You can display or work with them individually:\n",
    "\n",
    "for player, df in player_dfs.items():\n",
    "    print(f\"\\nData for {player}:\")\n",
    "    display(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_functions import his_player_defense_data, current_player_defense_data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#\"D:\\nba_player_csv_current\\season_2024-25\\all_quarters\\Alex Caruso_content.csv\"\n",
    "#\"D:\\nba_defense_csv_current\\defense_csv_2024-25\\all_quarter_defense_content.csv\"\n",
    "#\"D:\\nba_scheduled_csv\\schedule_csv_2025\\ATL_schedule_content.csv\"\n",
    "\n",
    "player_names = {'Shai Gilgeous-Alexander':'OKC', 'Alex Caruso':'OKC', 'Isaiah Hartenstein':'OKC'}\n",
    "date_list = ['2024-25']\n",
    "schedule_base_path = \"D:/nba_scheduled_csv/schedule_csv_2025/{schedule_team}_schedule_content.csv\"\n",
    "player_base_path = \"D:/nba_player_csv_current/season_{date}/all_quarters/{player}_content.csv\"\n",
    "defense_base_path = \"D:/nba_defense_csv_current/defense_csv_{date}/all_quarter_defense_content.csv\"\n",
    "# usage_data = f\"D:/nba_usage_csv_current/usage_csv_{date}/{date}_content.csv\"\n",
    "\n",
    "\n",
    "current_player_dic = {}\n",
    "\n",
    "for player, team in player_names.items():\n",
    "  current_player_frames = []\n",
    "\n",
    "  for date in date_list:\n",
    "    usage_data = pd.read_csv(f\"D:/nba_usage_csv_current/usage_csv_{date}/{date}_content.csv\")  \n",
    "\n",
    "    # use current_player function to to merge datat\n",
    "    merged_data, current_player_defense = current_player_defense_data(player_base_path,defense_base_path,schedule_base_path,player,date,team)\n",
    "\n",
    "    # Add the season column to usage_data\n",
    "    usage_data['season'] = date\n",
    "    \n",
    "    # Get the player's usage percentage from the usage data\n",
    "    # (Assumes that there is exactly one matching row)\n",
    "    player_usage = usage_data.loc[usage_data['Player'] == player, 'USG%'].values[0]\n",
    "    merged_data['USG'] = player_usage\n",
    "\n",
    "\n",
    "    # adding current player team pace\n",
    "    team_stat = current_player_defense.loc[current_player_defense['TEAM'] == team, 'PACE'].values[0]\n",
    "    merged_data[\"team_pace\"] = team_stat\n",
    "\n",
    "    # Exclude rows where the TEAM column matches the given team\n",
    "    merged_data = merged_data[merged_data['TEAM'] != team]\n",
    "\n",
    "\n",
    "    merged_data = merged_data[['season','Date', 'Matchup' ,'MIN_x', 'Team', 'TEAM', 'FGA', 'USG', 'DefRtg', 'PACE','team_pace']]\n",
    "\n",
    "    # Dropping duplicates\n",
    "    merged_data = merged_data.drop_duplicates()\n",
    "    \n",
    "    # Append the DataFrame for this date to the player's list\n",
    "    current_player_frames.append(merged_data)\n",
    "\n",
    "  # Combine all dates for the current player into one DataFrame\n",
    "  current_player_dic[player] = pd.concat(current_player_frames, ignore_index=True)\n",
    "\n",
    "# specific_player = 'Shai Gilgeous-Alexander'\n",
    "# for player, df in current_player_dic.items():\n",
    "#   if player == specific_player:\n",
    "#     print(f\"\\nData for {player}:\")\n",
    "#     display(df)\n",
    "\n",
    "\n",
    "specific_player = 'Shai Gilgeous-Alexander'\n",
    "for player, df in current_player_dic.items():\n",
    "  if player ==specific_player:\n",
    "    print(f\"\\nData for {player}:\")\n",
    "    df['FGA_rolling_3'] = df['FGA'].rolling(window=3).mean()\n",
    "\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "    # Selecting features and target\n",
    "    features = ['FGA_rolling_3', 'USG', 'MIN_x', 'DefRtg', 'PACE', 'team_pace']\n",
    "    target = 'FGA'\n",
    "\n",
    "    X = df[features]\n",
    "    y = df[target]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Model training\n",
    "    model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    cv_scores = cross_val_score(model, X, y, cv=5, scoring='neg_mean_squared_error')\n",
    "\n",
    "\n",
    "    print(f\"Length of df['Matchup']: {len(df['Matchup'])}\")\n",
    "    print(f\"Length of df['Date]: {len(df['Date'])}\")\n",
    "    print(f\"Length of y_pred: {len(y_pred)}\")\n",
    "    print(f\"Length of y_test: {len(y_test)}\")\n",
    "    matchups_test = df.loc[X_test.index, \"Matchup\"]\n",
    "    date_test = df.loc[X_test.index, \"Date\"]\n",
    "    print(f\"length of df['Matchup']: {len(matchups_test)}\")\n",
    "\n",
    "\n",
    "    predicted_vs_actual = pd.DataFrame({\n",
    "        \"Date\": date_test.values,  \n",
    "        \"Matchup\": matchups_test.values,  \n",
    "        \"Predicted Points\": y_pred.round(1), \n",
    "        \"Actual Points\": y_test}).reset_index(drop=True)\n",
    "    \n",
    "    print(predicted_vs_actual)\n",
    "\n",
    "    # Evaluate the model\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    print(f\"Mean Squared Error: {mse}\")\n",
    "    print(f\"RSME is: {rmse}\")\n",
    "    cv_rmse = np.sqrt(-cv_scores)\n",
    "\n",
    "    # Print the RMSE for each fold\n",
    "    print(f\"Cross-Validation RMSE for each fold: {cv_rmse}\")\n",
    "\n",
    "    # Print the average RMSE across all folds\n",
    "    print(f\"Average Cross-Validation RMSE: {cv_rmse.mean()}\")\n",
    "\n",
    "    display(df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Define paths for the player, defense, and schedule CSVs\n",
    "player_base_path = \"D:/nba_player_csv_historic/season_{date}/all_quarters/{player}_content.csv\"\n",
    "defense_base_path = \"D:/nba_defense_history_csv/defense_csv_{date}/all_quarter_defense_content.csv\"\n",
    "schedule_base_path = \"D:/nba_scheduled_csv/schedule_csv_2025/{schedule_team}_schedule_content.csv\"\n",
    "\n",
    "# Define parameters\n",
    "player = \"Shai Gilgeous-Alexander\"\n",
    "date = \"2023-24\"\n",
    "schedule_team = \"OKC\"  # Team for schedule filtering\n",
    "\n",
    "# Call the function\n",
    "merged_df_schedule, defense_current_player = current_player_defense_data(\n",
    "    player_base_path, defense_base_path, schedule_base_path, player, date, schedule_team\n",
    ")\n",
    "\n",
    "# **Displaying the merged data**\n",
    "print(\"Merged Data (Player + Defense + Schedule):\")\n",
    "print(merged_df_schedule.head())  # Show first few rows\n",
    "\n",
    "# **Displaying the extracted defense data (TEAM, PACE)**\n",
    "print(\"\\nDefense Data (TEAM, PACE):\")\n",
    "print(defense_current_player)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def predict_fga(player_data, alpha=0.3, forecast_periods=5):\n",
    "    \"\"\"\n",
    "    Predict future FGA (Field Goal Attempts) using EWMA\n",
    "    \n",
    "    Parameters:\n",
    "    player_data: pandas Series of historical FGA data\n",
    "    alpha: smoothing factor (default: 0.3)\n",
    "    forecast_periods: number of future games to predict\n",
    "    \n",
    "    Returns:\n",
    "    Dictionary containing:\n",
    "    - historical_ewma: EWMA values for historical data\n",
    "    - predictions: predicted FGA values\n",
    "    - confidence_intervals: upper and lower bounds for predictions\n",
    "    \"\"\"\n",
    "    # Calculate EWMA for historical data\n",
    "    historical_ewma = player_data.ewm(alpha=alpha, adjust=False).mean()\n",
    "    \n",
    "    # Calculate volatility (standard deviation of percentage changes)\n",
    "    volatility = player_data.pct_change().std()\n",
    "    \n",
    "    # Make predictions\n",
    "    last_value = historical_ewma.iloc[-1]\n",
    "    predictions = [last_value] * forecast_periods\n",
    "    \n",
    "    # Calculate confidence intervals (95%)\n",
    "    z_score = 1.96  # 95% confidence interval\n",
    "    std_error = volatility * np.sqrt(np.arange(1, forecast_periods + 1))\n",
    "    upper_bound = [last_value * (1 + z_score * se) for se in std_error]\n",
    "    lower_bound = [last_value * (1 - z_score * se) for se in std_error]\n",
    "    \n",
    "    return {\n",
    "        'historical_ewma': historical_ewma,\n",
    "        'predictions': predictions,\n",
    "        'confidence_intervals': {\n",
    "            'upper': upper_bound,\n",
    "            'lower': lower_bound\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Sample player FGA data for last 10 games\n",
    "    player_fga = pd.Series([\n",
    "        15, 18, 12, 16, 14, 19, 13, 17, 15, 16\n",
    "    ], name='FGA')\n",
    "    \n",
    "    # Make predictions\n",
    "    results = predict_fga(player_fga, alpha=0.3, forecast_periods=3)\n",
    "    \n",
    "    # Create analysis DataFrame\n",
    "    analysis = pd.DataFrame({\n",
    "        'Game': range(1, len(player_fga) + 1),\n",
    "        'Actual_FGA': player_fga,\n",
    "        'EWMA_FGA': results['historical_ewma']\n",
    "    })\n",
    "    \n",
    "    # Add predictions\n",
    "    prediction_df = pd.DataFrame({\n",
    "        'Game': range(len(player_fga) + 1, len(player_fga) + 4),\n",
    "        'Predicted_FGA': results['predictions'],\n",
    "        'Upper_Bound': results['confidence_intervals']['upper'],\n",
    "        'Lower_Bound': results['confidence_intervals']['lower']\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(prediction_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_functions import his_player_defense_data, current_player_defense_data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#\"D:\\nba_player_csv_current\\season_2024-25\\all_quarters\\Alex Caruso_content.csv\"\n",
    "#\"D:\\nba_defense_csv_current\\defense_csv_2024-25\\all_quarter_defense_content.csv\"\n",
    "#\"D:\\nba_scheduled_csv\\schedule_csv_2025\\ATL_schedule_content.csv\"\n",
    "\n",
    "player_names = {'Shai Gilgeous-Alexander':'OKC', 'Alex Caruso':'OKC', 'Isaiah Hartenstein':'OKC'}\n",
    "date_list = ['2022-23', \"2023-24\", \"2024-25\"]\n",
    "schedule_base_path = \"D:/nba_scheduled_csv/schedule_csv_2025/{schedule_team}_schedule_content.csv\"\n",
    "player_base_path = \"D:/nba_player_csv_historic/season_{date}/all_quarters/{player}_content.csv\"\n",
    "defense_base_path = \"D:/nba_defense_history_csv/defense_csv_{date}/all_quarter_defense_content.csv\"\n",
    "# usage_data = f\"D:/nba_usage_csv_current/usage_csv_{date}/{date}_content.csv\"\n",
    "\n",
    "\n",
    "current_player_dic = {}\n",
    "\n",
    "for player, team in player_names.items():\n",
    "  current_player_frames = []\n",
    "\n",
    "  for date in date_list:\n",
    "    usage_data = pd.read_csv(f\"D:/nba_usage_csv_historic/usage_csv_{date}/{date}_content.csv\")  \n",
    "\n",
    "    # use current_player function to to merge datat\n",
    "    merged_data, current_player_defense = his_player_defense_data(player_base_path,defense_base_path,player,date)\n",
    "\n",
    "    # Add the season column to usage_data\n",
    "    usage_data['season'] = date\n",
    "    \n",
    "    # Get the player's usage percentage from the usage data\n",
    "    # (Assumes that there is exactly one matching row)\n",
    "    player_usage = usage_data.loc[usage_data['Player'] == player, 'USG%'].values[0]\n",
    "    merged_data['USG'] = player_usage\n",
    "\n",
    "\n",
    "    # adding current player team pace\n",
    "    team_stat = current_player_defense.loc[current_player_defense['TEAM'] == team, 'PACE'].values[0]\n",
    "    merged_data[\"team_pace\"] = team_stat\n",
    "\n",
    "    # Exclude rows where the TEAM column matches the given team\n",
    "    merged_data = merged_data[merged_data['TEAM'] != team]\n",
    "    #display(merged_data.head(5))\n",
    "\n",
    "    merged_data = merged_data[['season','Date', 'Home/Away_game' ,'Matchup' ,'PTS','MIN_x', 'Team', 'TEAM', 'FGA', 'USG', 'DefRtg', 'PACE','team_pace']]\n",
    "\n",
    "    # Turn date into seconds\n",
    "    merged_data['Date_in_Seconds'] = pd.to_datetime(merged_data['Date']).astype('int64') // 10**9\n",
    "    merged_data = merged_data.sort_values(by=\"Date_in_Seconds\")\n",
    "\n",
    "    # Turn Home/Away game into 1 and 0\n",
    "    merged_data['home_away'] = merged_data['Home/Away_game'].apply(lambda x: 1 if x == 'Away' else 0)\n",
    "    # Dropping duplicates\n",
    "    merged_data = merged_data.drop_duplicates()\n",
    "    \n",
    "    # Append the DataFrame for this date to the player's list\n",
    "    current_player_frames.append(merged_data)\n",
    "\n",
    "  # Combine all dates for the current player into one DataFrame\n",
    "  current_player_dic[player] = pd.concat(current_player_frames, ignore_index=True)\n",
    "\n",
    "\n",
    "\n",
    "specific_player = 'Shai Gilgeous-Alexander'\n",
    "for player, df in current_player_dic.items():\n",
    "\n",
    "    print(f\"\\nData for {player}:\")\n",
    "    df['FGA_rolling_3'] = df['FGA'].rolling(window=3).mean()\n",
    "\n",
    "\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    from sklearn.model_selection import cross_val_score\n",
    "\n",
    "    alpha = 0.2\n",
    "\n",
    "    df['EWMA_FGA'] = df['FGA'].ewm(span=(2/(1-alpha)-1), adjust=False).mean()\n",
    "\n",
    "    df['EWMA_FGA_2'] = df['FGA'].ewm(span=(2/alpha - 1), adjust=False).mean()\n",
    "\n",
    "    alpha = 0.2  # Example smoothing factor\n",
    "    df['Exp_smooth'] = 0  # Initialize column\n",
    "\n",
    "    for i in range(1, len(df)):\n",
    "        df.loc[i, 'Exp_smooth'] = alpha * df.loc[i, 'FGA'] + (1 - alpha) * df.loc[i - 1, 'Exp_smooth']\n",
    "\n",
    "    display(df.head(24))\n",
    "\n",
    "    # # Selecting features and target\n",
    "    # features = ['USG', 'DefRtg', 'PACE', 'team_pace', 'home_away']\n",
    "    # target = 'FGA'\n",
    "\n",
    "    # X = df[features]\n",
    "    # y = df[target]\n",
    "\n",
    "    # timestamp = int(pd.Timestamp('12/31/2024').timestamp())\n",
    "\n",
    "\n",
    "    # train_data = df[df['Date_in_Seconds'] < timestamp]  # Training: First 4 seasons\n",
    "    # test_data = df[df['Date_in_Seconds'] >= timestamp]  # Testing: Most recent season\n",
    "\n",
    "    # # display(train_data)\n",
    "\n",
    "\n",
    "    # X_train = train_data[features]\n",
    "    # y_train = train_data[target]\n",
    "    # X_test = test_data[features]\n",
    "    # y_test = test_data[target]\n",
    "\n",
    "\n",
    "    # # X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # # Model training\n",
    "    # model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "    # model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "    # y_pred = model.predict(X_test)\n",
    "\n",
    "    # cv_scores = cross_val_score(model, X, y, cv=5, scoring='neg_mean_squared_error')\n",
    "\n",
    "\n",
    "    # print(f\"Length of df['Matchup']: {len(df['Matchup'])}\")\n",
    "    # print(f\"Length of df['Date]: {len(df['Date'])}\")\n",
    "    # print(f\"Length of y_pred: {len(y_pred)}\")\n",
    "    # print(f\"Length of y_test: {len(y_test)}\")\n",
    "    # matchups_test = df.loc[X_test.index, \"Matchup\"]\n",
    "    # date_test = df.loc[X_test.index, \"Date\"]\n",
    "    # print(f\"length of df['Matchup']: {len(matchups_test)}\")\n",
    "\n",
    "\n",
    "    # predicted_vs_actual = pd.DataFrame({\n",
    "    #     \"Date\": date_test.values,  \n",
    "    #     \"Matchup\": matchups_test.values,  \n",
    "    #     \"Predicted Points\": y_pred.round(1), \n",
    "    #     \"Actual Points\": y_test}).reset_index(drop=True)\n",
    "    \n",
    "    # print(predicted_vs_actual)\n",
    "\n",
    "    # # Evaluate the model\n",
    "    # mse = mean_squared_error(y_test, y_pred)\n",
    "    # rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    # print(f\"Mean Squared Error: {mse}\")\n",
    "    # print(f\"RSME is: {rmse}\")\n",
    "    # cv_rmse = np.sqrt(-cv_scores)\n",
    "\n",
    "    # # Print the RMSE for each fold\n",
    "    # print(f\"Cross-Validation RMSE for each fold: {cv_rmse}\")\n",
    "\n",
    "    # # Print the average RMSE across all folds\n",
    "    # print(f\"Average Cross-Validation RMSE: {cv_rmse.mean()}\")\n",
    "    # std_fga = np.std(df['FGA'])\n",
    "    # print(\"This is the std:\",std_fga)\n",
    "    \n",
    "    # latest_ewma_fga = df['EWMA_FGA'].iloc[-1]\n",
    "    # #display(df)\n",
    "\n",
    "    # # Create a DataFrame for the next game (fill with last known values)\n",
    "    # next_game_features = pd.DataFrame({\n",
    "    #     'USG': [df['USG'].iloc[-1]],  # Last known USG%\n",
    "    #     'DefRtg': \"116.2\",  # Last known DefRtg\n",
    "    #     'PACE': \"99.99\",  # Last known Pace\n",
    "    #     'team_pace': [df['team_pace'].iloc[-1]],  # Last known team pace\n",
    "    #     'home_away': [1]  # Assume it's an away game (or adjust accordingly)\n",
    "    # })\n",
    "\n",
    "    # display(next_game_features)\n",
    "\n",
    "\n",
    "    # # Predict next game's FGA\n",
    "    # predicted_fga_next_game = model.predict(next_game_features)[0]\n",
    "\n",
    "    # print(f\"Predicted FGA for Next Game: {predicted_fga_next_game:.2f}\")\n",
    "    # display(df.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_functions import his_player_defense_data, current_player_defense_data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#\"D:\\nba_player_csv_current\\season_2024-25\\all_quarters\\Alex Caruso_content.csv\"\n",
    "#\"D:\\nba_defense_csv_current\\defense_csv_2024-25\\all_quarter_defense_content.csv\"\n",
    "#\"D:\\nba_scheduled_csv\\schedule_csv_2025\\ATL_schedule_content.csv\"\n",
    "\n",
    "player_names = {'Shai Gilgeous-Alexander':'OKC', 'Alex Caruso':'OKC', 'Isaiah Hartenstein':'OKC'}\n",
    "date_list = ['2022-23', \"2023-24\", \"2024-25\"]\n",
    "schedule_base_path = \"D:/nba_scheduled_csv/schedule_csv_2025/{schedule_team}_schedule_content.csv\"\n",
    "player_base_path = \"D:/nba_player_csv_historic/season_{date}/all_quarters/{player}_content.csv\"\n",
    "defense_base_path = \"D:/nba_defense_history_csv/defense_csv_{date}/all_quarter_defense_content.csv\"\n",
    "# usage_data = f\"D:/nba_usage_csv_current/usage_csv_{date}/{date}_content.csv\"\n",
    "\n",
    "\n",
    "current_player_dic = {}\n",
    "\n",
    "for player, team in player_names.items():\n",
    "  current_player_frames = []\n",
    "\n",
    "  for date in date_list:\n",
    "    usage_data = pd.read_csv(f\"D:/nba_usage_csv_historic/usage_csv_{date}/{date}_content.csv\")  \n",
    "\n",
    "    # use current_player function to to merge datat\n",
    "    merged_data, current_player_defense = his_player_defense_data(player_base_path,defense_base_path,player,date)\n",
    "\n",
    "    # Add the season column to usage_data\n",
    "    usage_data['season'] = date\n",
    "    \n",
    "    # Get the player's usage percentage from the usage data\n",
    "    # (Assumes that there is exactly one matching row)\n",
    "    player_usage = usage_data.loc[usage_data['Player'] == player, 'USG%'].values[0]\n",
    "    merged_data['USG'] = player_usage\n",
    "\n",
    "\n",
    "    # adding current player team pace\n",
    "    team_stat = current_player_defense.loc[current_player_defense['TEAM'] == team, 'PACE'].values[0]\n",
    "    merged_data[\"team_pace\"] = team_stat\n",
    "\n",
    "    # Exclude rows where the TEAM column matches the given team\n",
    "    merged_data = merged_data[merged_data['TEAM'] != team]\n",
    "    #display(merged_data.head(5))\n",
    "\n",
    "    merged_data = merged_data[['season','Date', 'Home/Away_game' ,'Matchup' ,'PTS','MIN_x', 'Team', 'TEAM', 'FGA', 'USG', 'DefRtg', 'PACE','team_pace']]\n",
    "\n",
    "    # Turn date into seconds\n",
    "    merged_data['Date_in_Seconds'] = pd.to_datetime(merged_data['Date']).astype('int64') // 10**9\n",
    "    merged_data = merged_data.sort_values(by=\"Date_in_Seconds\")\n",
    "\n",
    "    # Turn Home/Away game into 1 and 0\n",
    "    merged_data['home_away'] = merged_data['Home/Away_game'].apply(lambda x: 1 if x == 'Away' else 0)\n",
    "    # Dropping duplicates\n",
    "    merged_data = merged_data.drop_duplicates()\n",
    "    \n",
    "    # Append the DataFrame for this date to the player's list\n",
    "    current_player_frames.append(merged_data)\n",
    "\n",
    "  # Combine all dates for the current player into one DataFrame\n",
    "  current_player_dic[player] = pd.concat(current_player_frames, ignore_index=True)\n",
    "\n",
    "specific_player = 'Shai Gilgeous-Alexander'\n",
    "\n",
    "for player, df in current_player_dic.items():\n",
    "  if player == specific_player:\n",
    "    df = df\n",
    "\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.linear_model import  LinearRegression\n",
    "    from sklearn.metrics import mean_absolute_error, r2_score\n",
    "\n",
    "\n",
    "    features = ['MIN_x','USG', 'DefRtg', 'PACE', 'team_pace', 'home_away']\n",
    "    target = 'FGA'\n",
    "\n",
    "    X = df[features]\n",
    "    y = df[target]\n",
    "\n",
    "    timestamp = int(pd.Timestamp('12/31/2024').timestamp())\n",
    "\n",
    "\n",
    "    train_data = df[df['Date_in_Seconds'] < timestamp]  # Training: First 4 seasons\n",
    "    test_data = df[df['Date_in_Seconds'] >= timestamp]  # Testing: Most recent season\n",
    "\n",
    "    # display(train_data)\n",
    "\n",
    "\n",
    "    X_train = train_data[features]\n",
    "    y_train = train_data[target]\n",
    "    X_test = test_data[features]\n",
    "    y_test = test_data[target]\n",
    "\n",
    "\n",
    "    # X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Model training\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Predict\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    print(\"MAE:\", mean_absolute_error(y_test, y_pred))\n",
    "    print(\"RÂ² Score:\", r2_score(y_test, y_pred))\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    print(\"RMSE:\", rmse)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_data_path(base_path, **Kwargs):\n",
    "    # Replace placeholders with actual values\n",
    "    for key, value in Kwargs.items():\n",
    "        base_path=base_path.replace(f\"{{{key}}}\", str(value))\n",
    "    return base_path\n",
    "\n",
    "base_path = \"/data/{year}/{month}/{day}/file.txt\"\n",
    "kwargs = {'year': 2025, 'month': '02', 'day': '10'}\n",
    "year = 2025\n",
    "build_data_path(base_path, year=year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_functions import his_player_defense_data, current_player_defense_data, build_data_path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "\n",
    "def fga_prediction_ewma(player_names: dict, date_list: list, usage_path,player_base_path,defense_base_path):\n",
    "    current_player_dic = {}\n",
    "\n",
    "    for player, team in player_names.items():\n",
    "        current_player_frames =[]\n",
    "\n",
    "        for date in date_list:\n",
    "            usage_path =build_data_path(usage_path,date)\n",
    "            usage_data = pd.read_csv(usage_path)\n",
    "\n",
    "            #merging player and defense dat into one\n",
    "            merged_data, current_defense_df = his_player_defense_data(player_base_path,defense_base_path,player,date)\n",
    "\n",
    "            #adding season to usage_data\n",
    "            usage_data['season'] = date\n",
    "\n",
    "            #Getting the player usage percentage for usage data and adding to merge\n",
    "            player_usage = usage_data.loc[usage_data['Player'] == player, 'USG%'].values[0]\n",
    "            merged_data['USG'] = player_usage\n",
    "\n",
    "            #adding the current player team pace\n",
    "            team_stat = current_defense_df.loc[current_defense_df['TEAM'] == team, 'PACE'].values[0]\n",
    "            merged_data[\"team_pace\"] = team_stat\n",
    "            \n",
    "            # Exclude rows where the TEAM column matches the given team\n",
    "            merged_data = merged_data[merged_data['TEAM'] != team]\n",
    "\n",
    "\n",
    "            merged_data = merged_data[['season','Date', 'Home/Away_game' ,'Matchup' ,'PTS','MIN_x', 'Team', 'TEAM', 'FGA', 'USG', 'DefRtg', 'PACE','team_pace']]\n",
    "\n",
    "            \n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:/nba_player_csv_historic/season_2022-23/all_quarters/Shai Gilgeous-Alexander_content.csv\n",
      "D:/nba_defense_history_csv/defense_csv_2022-23/all_quarter_defense_content.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TEAM</th>\n",
       "      <th>PACE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MIL</td>\n",
       "      <td>101.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BOS</td>\n",
       "      <td>99.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PHI</td>\n",
       "      <td>97.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DEN</td>\n",
       "      <td>98.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CLE</td>\n",
       "      <td>96.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>MEM</td>\n",
       "      <td>101.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>SAC</td>\n",
       "      <td>100.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NYK</td>\n",
       "      <td>97.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>BKN</td>\n",
       "      <td>98.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>PHX</td>\n",
       "      <td>98.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>GSW</td>\n",
       "      <td>102.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LAC</td>\n",
       "      <td>98.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>MIA</td>\n",
       "      <td>96.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LAL</td>\n",
       "      <td>101.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>MIN</td>\n",
       "      <td>101.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>NOP</td>\n",
       "      <td>99.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>ATL</td>\n",
       "      <td>101.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>TOR</td>\n",
       "      <td>97.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>CHI</td>\n",
       "      <td>99.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>OKC</td>\n",
       "      <td>101.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>DAL</td>\n",
       "      <td>97.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>UTA</td>\n",
       "      <td>101.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>IND</td>\n",
       "      <td>101.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>WAS</td>\n",
       "      <td>99.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>ORL</td>\n",
       "      <td>99.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>POR</td>\n",
       "      <td>99.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>CHA</td>\n",
       "      <td>101.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>HOU</td>\n",
       "      <td>99.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>SAS</td>\n",
       "      <td>102.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>DET</td>\n",
       "      <td>99.88</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   TEAM    PACE\n",
       "0   MIL  101.45\n",
       "1   BOS   99.15\n",
       "2   PHI   97.44\n",
       "3   DEN   98.74\n",
       "4   CLE   96.27\n",
       "5   MEM  101.50\n",
       "6   SAC  100.99\n",
       "7   NYK   97.75\n",
       "8   BKN   98.77\n",
       "9   PHX   98.83\n",
       "10  GSW  102.54\n",
       "11  LAC   98.84\n",
       "12  MIA   96.76\n",
       "13  LAL  101.92\n",
       "14  MIN  101.55\n",
       "15  NOP   99.58\n",
       "16  ATL  101.56\n",
       "17  TOR   97.85\n",
       "18  CHI   99.18\n",
       "19  OKC  101.94\n",
       "20  DAL   97.21\n",
       "21  UTA  101.02\n",
       "22  IND  101.68\n",
       "23  WAS   99.16\n",
       "24  ORL   99.66\n",
       "25  POR   99.25\n",
       "26  CHA  101.47\n",
       "27  HOU   99.74\n",
       "28  SAS  102.07\n",
       "29  DET   99.88"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyError",
     "evalue": "'OffRtg'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\mandy\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'OffRtg'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 45\u001b[0m\n\u001b[0;32m     42\u001b[0m display(current_player_defense)\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# adding current player team OffRtg\u001b[39;00m\n\u001b[1;32m---> 45\u001b[0m team_offrtg \u001b[38;5;241m=\u001b[39m \u001b[43mcurrent_player_defense\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcurrent_player_defense\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTEAM\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mteam\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mOffRtg\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mvalues[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     46\u001b[0m merged_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mteam_offrtg\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m team_offrtg\n\u001b[0;32m     48\u001b[0m \u001b[38;5;66;03m# Exclude rows where the TEAM column matches the given team\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\mandy\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\indexing.py:1184\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1182\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_scalar_access(key):\n\u001b[0;32m   1183\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_get_value(\u001b[38;5;241m*\u001b[39mkey, takeable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_takeable)\n\u001b[1;32m-> 1184\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_tuple\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1185\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1186\u001b[0m     \u001b[38;5;66;03m# we by definition only have the 0th axis\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m     axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxis \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\mandy\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\indexing.py:1368\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_tuple\u001b[1;34m(self, tup)\u001b[0m\n\u001b[0;32m   1366\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m suppress(IndexingError):\n\u001b[0;32m   1367\u001b[0m     tup \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_ellipsis(tup)\n\u001b[1;32m-> 1368\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_lowerdim\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtup\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1370\u001b[0m \u001b[38;5;66;03m# no multi-index, so validate all of the indexers\u001b[39;00m\n\u001b[0;32m   1371\u001b[0m tup \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_tuple_indexer(tup)\n",
      "File \u001b[1;32mc:\\Users\\mandy\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\indexing.py:1065\u001b[0m, in \u001b[0;36m_LocationIndexer._getitem_lowerdim\u001b[1;34m(self, tup)\u001b[0m\n\u001b[0;32m   1061\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tup):\n\u001b[0;32m   1062\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_label_like(key):\n\u001b[0;32m   1063\u001b[0m         \u001b[38;5;66;03m# We don't need to check for tuples here because those are\u001b[39;00m\n\u001b[0;32m   1064\u001b[0m         \u001b[38;5;66;03m#  caught by the _is_nested_tuple_indexer check above.\u001b[39;00m\n\u001b[1;32m-> 1065\u001b[0m         section \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1067\u001b[0m         \u001b[38;5;66;03m# We should never have a scalar section here, because\u001b[39;00m\n\u001b[0;32m   1068\u001b[0m         \u001b[38;5;66;03m#  _getitem_lowerdim is only called after a check for\u001b[39;00m\n\u001b[0;32m   1069\u001b[0m         \u001b[38;5;66;03m#  is_scalar_access, which that would be.\u001b[39;00m\n\u001b[0;32m   1070\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m section\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mndim:\n\u001b[0;32m   1071\u001b[0m             \u001b[38;5;66;03m# we're in the middle of slicing through a MultiIndex\u001b[39;00m\n\u001b[0;32m   1072\u001b[0m             \u001b[38;5;66;03m# revise the key wrt to `section` by inserting an _NS\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\mandy\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\indexing.py:1431\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_axis\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1429\u001b[0m \u001b[38;5;66;03m# fall thru to straight lookup\u001b[39;00m\n\u001b[0;32m   1430\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_key(key, axis)\n\u001b[1;32m-> 1431\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_label\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\mandy\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\indexing.py:1381\u001b[0m, in \u001b[0;36m_LocIndexer._get_label\u001b[1;34m(self, label, axis)\u001b[0m\n\u001b[0;32m   1379\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_label\u001b[39m(\u001b[38;5;28mself\u001b[39m, label, axis: AxisInt):\n\u001b[0;32m   1380\u001b[0m     \u001b[38;5;66;03m# GH#5567 this will fail if the label is not present in the axis.\u001b[39;00m\n\u001b[1;32m-> 1381\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mxs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\mandy\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\generic.py:4287\u001b[0m, in \u001b[0;36mNDFrame.xs\u001b[1;34m(self, key, axis, level, drop_level)\u001b[0m\n\u001b[0;32m   4285\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m axis \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4286\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m drop_level:\n\u001b[1;32m-> 4287\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m   4288\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\n\u001b[0;32m   4289\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\mandy\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mc:\\Users\\mandy\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'OffRtg'"
     ]
    }
   ],
   "source": [
    "from data_functions import his_player_defense_data, current_player_defense_data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#\"D:\\nba_player_csv_current\\season_2024-25\\all_quarters\\Alex Caruso_content.csv\"\n",
    "#\"D:\\nba_defense_csv_current\\defense_csv_2024-25\\all_quarter_defense_content.csv\"\n",
    "#\"D:\\nba_scheduled_csv\\schedule_csv_2025\\ATL_schedule_content.csv\"\n",
    "\n",
    "player_names = {'Shai Gilgeous-Alexander':'OKC', 'Alex Caruso':'OKC', 'Isaiah Hartenstein':'OKC'}\n",
    "date_list = [\"2022-23\",\"2023-24\",\"2024-25\"]\n",
    "schedule_base_path = \"D:/nba_scheduled_csv/schedule_csv_2025/{schedule_team}_schedule_content.csv\"\n",
    "player_base_path = \"D:/nba_player_csv_historic/season_{date}/all_quarters/{player}_content.csv\"\n",
    "defense_base_path = \"D:/nba_defense_history_csv/defense_csv_{date}/all_quarter_defense_content.csv\"\n",
    "# usage_data = f\"D:/nba_usage_csv_current/usage_csv_{date}/{date}_content.csv\"\n",
    "\n",
    "\n",
    "\n",
    "current_player_dic = {}\n",
    "\n",
    "for player, team in player_names.items():\n",
    "  current_player_frames = []\n",
    "\n",
    "  for date in date_list:\n",
    "    usage_data = pd.read_csv(f\"D:/nba_usage_csv_historic/usage_csv_{date}/{date}_content.csv\")  \n",
    "\n",
    "    # use current_player function to to merge datat\n",
    "    merged_data, current_player_defense = his_player_defense_data(player_base_path,defense_base_path,player,date)\n",
    "\n",
    "    # Add the season column to usage_data\n",
    "    usage_data['season'] = date\n",
    "    \n",
    "    # Get the player's usage percentage from the usage data\n",
    "    # (Assumes that there is exactly one matching row)\n",
    "    player_usage = usage_data.loc[usage_data['Player'] == player, 'USG%'].values[0]\n",
    "    merged_data['USG'] = player_usage\n",
    "\n",
    "\n",
    "    # adding current player team pace\n",
    "    team_stat = current_player_defense.loc[current_player_defense['TEAM'] == team, 'PACE'].values[0]\n",
    "    merged_data[\"team_pace\"] = team_stat\n",
    "\n",
    "    display(current_player_defense)\n",
    "\n",
    "    # adding current player team OffRtg\n",
    "    team_offrtg = current_player_defense.loc[current_player_defense['TEAM'] == team, 'OffRtg'].values[0]\n",
    "    merged_data[\"team_offrtg\"] = team_offrtg\n",
    "\n",
    "    # Exclude rows where the TEAM column matches the given team\n",
    "    merged_data = merged_data[merged_data['TEAM'] != team]\n",
    "    #display(merged_data.head(5))\n",
    "\n",
    "    # merged_data = merged_data[['season','Date', 'Home/Away_game' ,'Matchup' ,'PTS','MIN_x', 'Team', 'TEAM', 'FGA', 'USG', 'DefRtg', 'PACE','team_pace']]\n",
    "\n",
    "    # Turn date into seconds\n",
    "    merged_data['Date_in_Seconds'] = pd.to_datetime(merged_data['Date']).astype('int64') // 10**9\n",
    "    merged_data = merged_data.sort_values(by=\"Date_in_Seconds\")\n",
    "\n",
    "    # Turn Home/Away game into 1 and 0\n",
    "    merged_data['home_away'] = merged_data['Home/Away_game'].apply(lambda x: 1 if x == 'Away' else 0)\n",
    "    # Dropping duplicates\n",
    "    merged_data = merged_data.drop_duplicates()\n",
    "    \n",
    "    # Append the DataFrame for this date to the player's list\n",
    "    current_player_frames.append(merged_data)\n",
    "\n",
    "  # Combine all dates for the current player into one DataFrame\n",
    "  current_player_dic[player] = pd.concat(current_player_frames, ignore_index=True)\n",
    "\n",
    "\n",
    "\n",
    "specific_player = 'Shai Gilgeous-Alexander'\n",
    "for player, df in current_player_dic.items():\n",
    "\n",
    "    print(f\"\\nData for {player}:\")\n",
    "    df['FGA_rolling_3'] = df['FGA'].rolling(window=3).mean()\n",
    "\n",
    "\n",
    "\n",
    "    alpha = 0.2\n",
    "\n",
    "    df['EWMA_FGA'] = df['FGA'].ewm(span=(2/(1-alpha)-1), adjust=False).mean()\n",
    "\n",
    "    df['EWMA_FGA_2'] = df['FGA'].ewm(span=(2/alpha - 1), adjust=False).mean()\n",
    "\n",
    "    # alpha = 0.2  # Example smoothing factor\n",
    "    df['Exp_smooth'] = 21  # Initialize column\n",
    "\n",
    "    for i in range(1, len(df)):\n",
    "        df.loc[i, 'Exp_smooth'] = alpha * df.loc[i, 'FGA'] + (1 - alpha) * df.loc[i - 1, 'Exp_smooth']\n",
    "\n",
    "    df_act = df\n",
    "\n",
    "    # display(df_act)\n",
    "\n",
    "    df\n",
    "\n",
    "    value_fga_list = []\n",
    "    moving_average_list = []\n",
    "    next_next_value_list = []\n",
    "    date_list = []\n",
    "    \n",
    "    for value_fga, moving_average, date in zip(df['FGA'],df['EWMA_FGA_2'],df['Date']):\n",
    "       value_fga_list.append(value_fga)\n",
    "       moving_average_list.append(moving_average)\n",
    "       #print(\"this is actual:\",value_fga, \"this is last_predicted\",moving_average)\n",
    "       next_next_value = alpha * value_fga + (1 - alpha) * moving_average\n",
    "       next_next_value_list.append(next_next_value)\n",
    "       date_list.append(date)\n",
    "       #print(\"this is next:\",next_next_value)\n",
    "       dataframe_dic= {'Date':date_list,'Actual_FGA': value_fga_list, 'Moving_average':moving_average_list, 'Next':next_next_value_list}\n",
    "       dataframe = pd.DataFrame(dataframe_dic)\n",
    "       dataframe['Next'] = dataframe['Next'].shift(1)\n",
    "\n",
    "    #display(dataframe)\n",
    "\n",
    "\n",
    "    # Predict the next (11th) value\n",
    "    last_actual = df['FGA'].iloc[-1]  # Last known FGA\n",
    "    last_smoothed = df['EWMA_FGA_2'].iloc[-1]  # Last smoothed value\n",
    "    next_value = alpha * last_actual + (1 - alpha) * last_smoothed\n",
    "\n",
    "\n",
    "    # Print and add the prediction\n",
    "    print(f\"Predicted value: {next_value}\")\n",
    "\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    from sklearn.model_selection import cross_val_score\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "\n",
    "\n",
    "    features = ['EWMA_FGA_2', 'PACE', 'team_pace', 'USG', 'DefRtg','MIN_x', 'home_away', 'Date_in_Seconds','OffRtg', 'team_offrtg']\n",
    "    target = 'FGA'\n",
    "\n",
    "\n",
    "     # Continue with your existing operations\n",
    "    timestamp = int(pd.Timestamp('2024-12-31').timestamp())\n",
    "    train_data = df[df['Date_in_Seconds'] < timestamp]  # Replace '2023-01-01' with the corresponding timestamp\n",
    "    test_data =  df[df['Date_in_Seconds'] >= timestamp]  # Replace '2023-01-01' with the corresponding timestamp\n",
    "\n",
    "\n",
    "    X_train = train_data[features]\n",
    "    y_train = train_data[target]\n",
    "    X_test = test_data[features]\n",
    "    y_test = test_data[target]\n",
    "\n",
    "\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train,y_train)\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "     # Compare predictions with actual points\n",
    "    predicted_vs_actual = pd.DataFrame({\n",
    "        \"Date\": test_data['Date'],  \n",
    "        \"Matchup\": test_data['Matchup'],  \n",
    "        \"Predicted Points\": y_pred.round(1), \n",
    "        \"Actual Points\": y_test}).reset_index(drop=True)\n",
    "    \n",
    "    display(predicted_vs_actual)\n",
    "\n",
    "    # X_future = pd.DataFrame({\n",
    "    # 'MIN': result['Min'],         \n",
    "    # 'FGA': result['predicted_fga'],         \n",
    "    # 'FG%': result['predicted_fg_percentage'],         \n",
    "    # 'FTA': result['predicted_fta'],          \n",
    "    # 'DefRtg': result['DefRtg']    \n",
    "    # })\n",
    "\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "\n",
    "    print(f\"MAE: {mae}, RMSE: {rmse}\")\n",
    "\n",
    "    import numpy as np\n",
    "\n",
    "    # Drop NaN values (first row of 'Next' is NaN due to shift)\n",
    "    rmse_df = dataframe.dropna(subset=['Actual_FGA', 'Next'])\n",
    "\n",
    "    # Compute RMSE\n",
    "    rmse = np.sqrt(((rmse_df['Actual_FGA'] - rmse_df['Next']) ** 2).mean())\n",
    "\n",
    "    print(\"RMSE:\", rmse)\n",
    "\n",
    "\n",
    "    # Drop NaN values (due to shift)\n",
    "    comparison_df = dataframe.dropna(subset=['Actual_FGA', 'Next'])\n",
    "\n",
    "    # Total number of valid comparisons\n",
    "    total = len(comparison_df)\n",
    "\n",
    "    # Count occurrences\n",
    "    higher_count = (comparison_df['Next'] > comparison_df['Actual_FGA']).sum()\n",
    "    lower_count = (comparison_df['Next'] < comparison_df['Actual_FGA']).sum()\n",
    "    equal_count = (comparison_df['Next'] == comparison_df['Actual_FGA']).sum()\n",
    "\n",
    "    # Calculate percentages\n",
    "    higher_percent = (higher_count / total) * 100\n",
    "    lower_percent = (lower_count / total) * 100\n",
    "    equal_percent = (equal_count / total) * 100\n",
    "\n",
    "    # Print results\n",
    "    print(f\"Percentage of predictions HIGHER than actual: {higher_percent:.2f}%\")\n",
    "    print(f\"Percentage of predictions LOWER than actual: {lower_percent:.2f}%\")\n",
    "    print(f\"Percentage of predictions EQUAL to actual: {equal_percent:.2f}%\")\n",
    "\n",
    "\n",
    "    comparison_df = dataframe.dropna(subset=['Actual_FGA', 'Next'])\n",
    "\n",
    "    # Filter cases where Next is higher than Actual\n",
    "    higher_cases = comparison_df[comparison_df['Next'] > comparison_df['Actual_FGA']]\n",
    "    higher_difference_avg = (higher_cases['Next'] - higher_cases['Actual_FGA']).mean()\n",
    "\n",
    "    # Filter cases where Next is lower than Actual\n",
    "    lower_cases = comparison_df[comparison_df['Next'] < comparison_df['Actual_FGA']]\n",
    "    lower_difference_avg = (lower_cases['Actual_FGA'] - lower_cases['Next']).mean()\n",
    "\n",
    "    # Print results\n",
    "    print(f\"Average difference when prediction is HIGHER: {higher_difference_avg:.2f}\")\n",
    "    print(f\"Average difference when prediction is LOWER: {lower_difference_avg:.2f}\")\n",
    "\n",
    "    # Drop NaN values\n",
    "    comparison_df = dataframe.dropna(subset=['Actual_FGA', 'Next'])\n",
    "\n",
    "    # Compute Mean Error (ME)\n",
    "    mean_error = (comparison_df['Next'] - comparison_df['Actual_FGA']).mean()\n",
    "\n",
    "    # Print result\n",
    "    print(f\"Mean Error (ME): {mean_error:.2f}\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
